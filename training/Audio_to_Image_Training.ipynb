
# Install necessary Python packages
!pip install -q datasets transformers torch librosa tqdm accelerate pandas scikit-learn

# Import the Google Drive library and mount your drive
from google.colab import drive
drive.mount('/content/drive')
print("Drive mounted successfully.")

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import ClapModel, ClapProcessor, CLIPTextModel, AutoTokenizer
import librosa
from tqdm.auto import tqdm
import pandas as pd
from sklearn.model_selection import train_test_split

# --- Configuration ---
# Set the device to GPU if available, otherwise CPU
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# Training Hyperparameters
NUM_EPOCHS = 10
BATCH_SIZE = 16
LEARNING_RATE = 1e-4

# Paths on your Google Drive
DRIVE_PROJECT_PATH = "/content/drive/MyDrive/AudioToImageProject"
AUDIO_CACHE_DIR = os.path.join(DRIVE_PROJECT_PATH, "audio_cache")
MODEL_SAVE_PATH = os.path.join(DRIVE_PROJECT_PATH, "projection_network1.pth")
METADATA_CSV_PATH = os.path.join(DRIVE_PROJECT_PATH, "esc50_cleaned.csv")

os.makedirs(AUDIO_CACHE_DIR, exist_ok=True)

# Model Identifiers
CLAP_MODEL_ID = "laion/clap-htsat-unfused"
SD_MODEL_ID = "runwayml/stable-diffusion-v1-5"


# --- TRANSFORMER-BASED PROJECTION NETWORK ---
class ProjectionNetwork(nn.Module):
    """
    A Transformer-based network to project a single audio embedding
    into a sequence of 77 text-conditioning embeddings.
    """
    def __init__(self, audio_embedding_dim=512, text_embedding_dim=768, num_decoder_layers=4, nhead=4):
        super().__init__()
        self.text_embedding_dim = text_embedding_dim
        self.input_proj = nn.Linear(audio_embedding_dim, text_embedding_dim)
        self.start_token = nn.Parameter(torch.randn(1, 1, text_embedding_dim))
        self.positional_encoding = nn.Parameter(torch.randn(1, 77, text_embedding_dim))
        decoder_layer = nn.TransformerDecoderLayer(d_model=text_embedding_dim, nhead=nhead, batch_first=True)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)

    def forward(self, audio_embedding):
        memory = self.input_proj(audio_embedding).unsqueeze(1)
        batch_size = audio_embedding.shape[0]
        tgt_seq = self.start_token.expand(batch_size, 77, -1)
        tgt_with_pos = tgt_seq + self.positional_encoding
        output_sequence = self.transformer_decoder(tgt_with_pos, memory)
        return output_sequence

class CustomAudioDataset(Dataset):
    """
    This Dataset loads audio and captions from a pandas DataFrame.
    """
    def __init__(self, metadata_df, audio_cache_dir, clap_processor, sd_tokenizer):
        self.audio_cache_dir = audio_cache_dir
        self.clap_processor = clap_processor
        self.sd_tokenizer = sd_tokenizer
        self.target_sr = 48000
        self.metadata = metadata_df # Use the passed DataFrame

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata.iloc[idx]
        caption = item['caption']
        clip_filename = item['file_name']
        audio_path = os.path.join(self.audio_cache_dir, clip_filename)

        try:
            waveform, _ = librosa.load(audio_path, sr=self.target_sr, mono=True)
            audio_features = self.clap_processor(audio=waveform, sampling_rate=self.target_sr, return_tensors="pt")
        except Exception as e:
            print(f"WARNING: Error loading {audio_path}. Skipping. Error: {e}")
            return self.__getitem__((idx + 1) % len(self))

        text_inputs = self.sd_tokenizer(caption, padding="max_length", truncation=True, return_tensors="pt")

        return {
            "audio_features": audio_features.input_features.squeeze(0),
            "input_ids": text_inputs.input_ids.squeeze(0),
            "attention_mask": text_inputs.attention_mask.squeeze(0)
        }

def main():
    # --- 1. Load Frozen Models ---
    print("Loading frozen CLAP and Stable Diffusion models...")
    clap_model = ClapModel.from_pretrained(CLAP_MODEL_ID).to(DEVICE)
    clap_processor = ClapProcessor.from_pretrained(CLAP_MODEL_ID)
    sd_text_encoder = CLIPTextModel.from_pretrained(SD_MODEL_ID, subfolder="text_encoder").to(DEVICE)
    sd_tokenizer = AutoTokenizer.from_pretrained(SD_MODEL_ID, subfolder="tokenizer")

    for param in clap_model.parameters(): param.requires_grad = False
    for param in sd_text_encoder.parameters(): param.requires_grad = False
    print("Frozen models loaded.")

    # --- 2. Setup Dataset and Dataloaders with Validation Split ---
    print("Setting up dataset and dataloaders...")
    try:
        metadata_df = pd.read_csv(METADATA_CSV_PATH)
    except FileNotFoundError:
        raise RuntimeError(f"Metadata file not found at {METADATA_CSV_PATH}!")

    # --- Split the data into training and validation sets (90/10 split) ---
    train_df, val_df = train_test_split(metadata_df, test_size=0.1, random_state=42)
    print(f"Data split into {len(train_df)} training samples and {len(val_df)} validation samples.")

    train_dataset = CustomAudioDataset(train_df, AUDIO_CACHE_DIR, clap_processor, sd_tokenizer)
    val_dataset = CustomAudioDataset(val_df, AUDIO_CACHE_DIR, clap_processor, sd_tokenizer)

    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, persistent_workers=True)
    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, persistent_workers=True)
    print("Dataset and dataloaders ready.")

    # --- 3. Initialize Model, Optimizer, and Loss ---
    projection_net = ProjectionNetwork().to(DEVICE)

    if os.path.exists(MODEL_SAVE_PATH):
        print(f"Found saved model at {MODEL_SAVE_PATH}. Loading weights...")
        try:
            projection_net.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))
            print("Weights loaded successfully. Resuming training.")
        except RuntimeError as e:
            print(f"Error loading saved weights: {e}\nStarting training from scratch.")

    optimizer = torch.optim.AdamW(projection_net.parameters(), lr=LEARNING_RATE)
    loss_fn = nn.MSELoss()
    print("Training setup is ready.")

    # --- 4. The Training Loop with Validation ---
    print(f"\nStarting training for {NUM_EPOCHS} epochs...")
    best_val_loss = float('inf')

    for epoch in range(NUM_EPOCHS):
        # --- TRAINING PHASE ---
        projection_net.train()
        total_train_loss = 0.0
        for batch in tqdm(train_dataloader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Training]"):
            audio_features = batch["audio_features"].to(DEVICE)
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)

            with torch.no_grad():
                target_text_outputs = sd_text_encoder(input_ids=input_ids, attention_mask=attention_mask)
                target_text_embedding = target_text_outputs.last_hidden_state
                audio_embedding = clap_model.get_audio_features(input_features=audio_features)

            optimizer.zero_grad()
            projected_embedding = projection_net(audio_embedding)
            loss = loss_fn(projected_embedding, target_text_embedding)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_dataloader)
        print(f"Epoch {epoch + 1} | Avg Training Loss: {avg_train_loss:.6f}")

        # --- VALIDATION PHASE ---
        projection_net.eval()
        total_val_loss = 0.0
        with torch.no_grad():
            for batch in tqdm(val_dataloader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS} [Validation]"):
                audio_features = batch["audio_features"].to(DEVICE)
                input_ids = batch["input_ids"].to(DEVICE)
                attention_mask = batch["attention_mask"].to(DEVICE)

                target_text_outputs = sd_text_encoder(input_ids=input_ids, attention_mask=attention_mask)
                target_text_embedding = target_text_outputs.last_hidden_state
                audio_embedding = clap_model.get_audio_features(input_features=audio_features)

                projected_embedding = projection_net(audio_embedding)
                loss = loss_fn(projected_embedding, target_text_embedding)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        print(f"Epoch {epoch + 1} | Avg Validation Loss: {avg_val_loss:.6f}")

        # --- SAVE BASED ON VALIDATION LOSS ---
        if avg_val_loss < best_val_loss:
            print(f"New best model found! (Validation loss improved from {best_val_loss:.6f} to {avg_val_loss:.6f})")
            best_val_loss = avg_val_loss
            print(f"Saving model checkpoint to {MODEL_SAVE_PATH}")
            torch.save(projection_net.state_dict(), MODEL_SAVE_PATH)

    print("\nTraining finished!")

# Run the main function
if __name__ == '__main__':
    main()
